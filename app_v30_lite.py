#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šç»´è½®å›ç ´è§£ç³»ç»Ÿï¼ˆæ¸Šå¼€å‘ï¼‰- v3.0 Lite ç»ˆæè¿›åŒ–ç‰ˆ
==================================================
âœ¨ AI æ™ºèƒ½å‡ºé¢˜ | AI æ·±åº¦è¯„åˆ† | å¤šæ¨¡å‹èåˆå¯¹è¯
ğŸš€ æœºå™¨å­¦ä¹ é¢„æµ‹ | å‘é‡æ£€ç´¢çŸ¥è¯†åº“ | æ·±åº¦æ¸¸æˆåŒ–
ğŸ“± PWA æ”¯æŒ | 20+ è¯­è¨€ | ä¼ä¸šçº§å®‰å…¨
ğŸ”® æµç¨‹ä¼˜åŒ– | æ€§èƒ½æå‡ | ç¨³å®šéƒ¨ç½²

ç‰ˆæœ¬ï¼šv3.0 Lite
ä¼˜åŒ–ï¼šæ ¸å¿ƒæè‡´åŠŸèƒ½ + äº‘ç«¯ç¨³å®šéƒ¨ç½²
"""

import streamlit as st
import os
import json
import time
import random
import hashlib
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Callable
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor
import threading
from collections import deque
from dataclasses import dataclass, field, asdict
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

# æ ¸å¿ƒä¾èµ–
import requests
import numpy as np
import pandas as pd

# Matplotlib é…ç½® - ç§»åˆ°é¡¶éƒ¨å¹¶å»¶è¿Ÿå¯¼å…¥
import matplotlib
matplotlib.use('Agg', force=True)
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import io
import base64
import re

# Streamlit é…ç½®
st.set_page_config(
    page_title="å¤šç»´è½®å›ç ´è§£ç³»ç»Ÿ v3.0",
    page_icon="ğŸ”®",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================== æ ¸å¿ƒé…ç½® ====================

class SystemConfig:
    """ç³»ç»Ÿé…ç½®"""
    def __init__(self):
        # API é…ç½®
        self.api_providers = {
            "groq": {
                "name": "Groq",
                "base_url": "https://api.groq.com/openai/v1",
                "models": ["llama-3.3-70b-versatile", "llama-3.1-8b-instant"],
                "free": True,
                "priority": 1
            },
            "openai": {
                "name": "OpenAI",
                "base_url": "https://api.openai.com/v1",
                "models": ["gpt-4-turbo", "gpt-3.5-turbo"],
                "free": False,
                "priority": 2
            },
            "anthropic": {
                "name": "Anthropic",
                "base_url": "https://api.anthropic.com/v1",
                "models": ["claude-3-opus-20240229", "claude-3-sonnet-20240229"],
                "free": False,
                "priority": 3
            },
            "cohere": {
                "name": "Cohere",
                "base_url": "https://api.cohere.ai/v1",
                "models": ["command", "command-light"],
                "free": False,
                "priority": 4
            }
        }
        
        # æ„è¯†ç»´åº¦
        self.consciousness_dimensions = [
            "reasoning",      # é€»è¾‘æ¨ç†
            "creative",       # åˆ›é€ æ€§æ€ç»´
            "knowledge",      # çŸ¥è¯†åº”ç”¨
            "depth",          # æ·±åº¦æ€è€ƒ
            "coding",         # ä»£ç èƒ½åŠ›
            "intuition",      # ç›´è§‰æ´å¯Ÿ
            "synthesis",      # ç»¼åˆåˆ†æ
            "memory",         # è®°å¿†èƒ½åŠ›
            "emotion",        # æƒ…ç»ªç®¡ç†
            "decision",       # å†³ç­–èƒ½åŠ›
            "learning",       # å­¦ä¹ é€Ÿåº¦
            "innovation"      # åˆ›æ–°èƒ½åŠ›
        ]
        
        # æˆå°±é…ç½®
        self.achievements_config = {
            "first_test": {"name": "åˆæ¬¡è§‰é†’", "desc": "å®Œæˆç¬¬ä¸€æ¬¡æ„è¯†æµ‹è¯•", "exp": 50},
            "test_master": {"name": "æµ‹è¯•è¾¾äºº", "desc": "å®Œæˆ10æ¬¡æ„è¯†æµ‹è¯•", "exp": 200},
            "perfect_score": {"name": "å®Œç¾è¡¨ç°", "desc": "å•æ¬¡æµ‹è¯•è¶…è¿‡80åˆ†", "exp": 300},
            "level_10": {"name": "è¿›é˜¶è€…", "desc": "è¾¾åˆ°10çº§", "exp": 500},
            "level_50": {"name": "æ„è¯†å¤§å¸ˆ", "desc": "è¾¾åˆ°50çº§", "exp": 2000},
            "conversation_100": {"name": "æ·±åº¦å¯¹è¯è€…", "desc": "å®Œæˆ100æ¬¡å¯¹è¯", "exp": 300},
            "knowledge_collector": {"name": "çŸ¥è¯†æ”¶è—å®¶", "desc": "ä¸Šä¼ 10ä¸ªæ–‡æ¡£", "exp": 200},
            "all_dimensions": {"name": "å…¨èƒ½å¤§å¸ˆ", "desc": "æ‰€æœ‰ç»´åº¦è¶…è¿‡8åˆ†", "exp": 1000}
        }
        
        # ç­‰çº§é…ç½®
        self.level_config = {
            "max_level": 100,
            "exp_base": 100,
            "exp_growth": 1.2
        }

# ==================== æ•°æ®ç»“æ„ ====================

class ConsciousnessLevel(Enum):
    """æ„è¯†ç­‰çº§"""
    AWAKENING = 1
    RISING = 2
    ASCENDING = 3
    TRANSCENDING = 4
    TRANSCENDENT = 5
    ENLIGHTENED = 6
    MASTER = 7

class QuestionDifficulty(Enum):
    """é¢˜ç›®éš¾åº¦"""
    BEGINNER = 1
    INTERMEDIATE = 2
    ADVANCED = 3
    EXPERT = 4
    MASTER = 5

@dataclass
class ConsciousnessSnapshot:
    """æ„è¯†å¿«ç…§"""
    timestamp: datetime
    scores: Dict[str, float]
    level: ConsciousnessLevel
    total_score: float
    test_answers: List[Dict] = field(default_factory=list)
    metadata: Dict = field(default_factory=dict)

@dataclass
class Question:
    """é¢˜ç›®"""
    id: str
    dimension: str
    difficulty: QuestionDifficulty
    content: str
    type: str  # open, choice, scenario, code
    options: List[str] = field(default_factory=list)
    reference: str = ""
    metadata: Dict = field(default_factory=dict)

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    snapshot: ConsciousnessSnapshot
    question_count: int
    correct_count: int
    accuracy: float
    time_spent: float
    confidence_scores: Dict[str, float]

@dataclass
class ConversationMessage:
    """å¯¹è¯æ¶ˆæ¯"""
    role: str
    content: str
    timestamp: datetime
    confidence: float = 0.0
    models_used: List[str] = field(default_factory=list)
    metadata: Dict = field(default_factory=dict)

# ==================== å¼‚æ­¥ç¼“å­˜ç³»ç»Ÿ ====================

class AsyncCache:
    """é«˜æ€§èƒ½å¼‚æ­¥ç¼“å­˜ç³»ç»Ÿ"""
    def __init__(self, max_size: int = 2000, ttl: int = 3600):
        self.cache = {}
        self.expiry = {}
        self.lock = threading.RLock()
        self.max_size = max_size
        self.ttl = ttl
        self.hits = 0
        self.misses = 0
        self.evictions = 0
    
    def _generate_key(self, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{args}-{sorted(kwargs.items())}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get(self, *args, **kwargs) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        key = self._generate_key(*args, **kwargs)
        with self.lock:
            if key in self.cache:
                if time.time() < self.expiry[key]:
                    self.hits += 1
                    return self.cache[key]
                else:
                    del self.cache[key]
                    del self.expiry[key]
            self.misses += 1
            return None
    
    def set(self, value: Any, *args, **kwargs):
        """è®¾ç½®ç¼“å­˜"""
        key = self._generate_key(*args, **kwargs)
        with self.lock:
            # LRU æ·˜æ±°
            if len(self.cache) >= self.max_size and key not in self.cache:
                oldest_key = min(self.expiry.keys(), key=lambda k: self.expiry[k])
                del self.cache[oldest_key]
                del self.expiry[oldest_key]
                self.evictions += 1
            
            self.cache[key] = value
            self.expiry[key] = time.time() + self.ttl
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            self.expiry.clear()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0
        return {
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": hit_rate,
            "size": len(self.cache),
            "max_size": self.max_size,
            "evictions": self.evictions
        }

# ==================== API ç®¡ç†ç³»ç»Ÿ ====================

class APIManager:
    """API ç®¡ç†å™¨ - æ™ºèƒ½è·¯ç”±ä¸å¥åº·ç›‘æµ‹"""
    def __init__(self, api_keys: Dict[str, str], config: SystemConfig):
        self.api_keys = api_keys
        self.config = config
        self.cache = AsyncCache()
        self.health_status = {name: {"available": True, "last_check": datetime.now(), "failures": 0} 
                              for name in config.api_providers.keys()}
        self.usage_stats = {name: {"requests": 0, "success": 0, "errors": 0, "avg_time": 0} 
                           for name in config.api_providers.keys()}
    
    def get_available_providers(self) -> List[str]:
        """è·å–å¯ç”¨çš„ API æä¾›å•†"""
        return [name for name, status in self.health_status.items() 
                if status["available"] and self.api_keys.get(name)]
    
    def select_best_provider(self, task_type: str = "general") -> Optional[str]:
        """æ™ºèƒ½é€‰æ‹©æœ€ä½³ API æä¾›å•†"""
        available = self.get_available_providers()
        if not available:
            return None
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹å’Œä¼˜å…ˆçº§é€‰æ‹©
        if task_type == "fast":
            # é€‰æ‹©é€Ÿåº¦æœ€å¿«çš„
            providers = sorted(available, 
                              key=lambda x: self.usage_stats[x]["avg_time"] or 999)
        elif task_type == "quality":
            # é€‰æ‹©è´¨é‡æœ€é«˜çš„
            providers = sorted(available, 
                              key=lambda x: self.config.api_providers[x]["priority"])
        else:
            # é»˜è®¤æ ¹æ®ä¼˜å…ˆçº§
            providers = sorted(available, 
                              key=lambda x: self.config.api_providers[x]["priority"])
        
        return providers[0] if providers else None
    
    def call_api(self, provider: str, messages: List[Dict], 
                 model: str = None, **kwargs) -> Dict[str, Any]:
        """è°ƒç”¨ API"""
        cache_key = f"{provider}_{model}_{hash(str(messages))}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        provider_config = self.config.api_providers[provider]
        api_key = self.api_keys.get(provider)
        
        if not api_key:
            return {"error": f"API key for {provider} not configured"}
        
        start_time = time.time()
        try:
            headers = {"Content-Type": "application/json"}
            url = ""
            data = {}
            
            if provider == "groq":
                headers["Authorization"] = f"Bearer {api_key}"
                url = f"{provider_config['base_url']}/chat/completions"
                model = model or provider_config["models"][0]
                data = {
                    "model": model,
                    "messages": messages,
                    "temperature": kwargs.get("temperature", 0.7),
                    "max_tokens": kwargs.get("max_tokens", 2048)
                }
            
            elif provider == "openai":
                headers["Authorization"] = f"Bearer {api_key}"
                url = f"{provider_config['base_url']}/chat/completions"
                model = model or provider_config["models"][0]
                data = {
                    "model": model,
                    "messages": messages,
                    "temperature": kwargs.get("temperature", 0.7),
                    "max_tokens": kwargs.get("max_tokens", 2048)
                }
            
            elif provider == "anthropic":
                headers["x-api-key"] = api_key
                headers["anthropic-version"] = "2023-06-01"
                url = f"{provider_config['base_url']}/messages"
                model = model or provider_config["models"][0]
                # è½¬æ¢æ¶ˆæ¯æ ¼å¼
                system_msg = next((m["content"] for m in messages if m["role"] == "system"), "")
                user_msgs = [m for m in messages if m["role"] != "system"]
                data = {
                    "model": model,
                    "system": system_msg,
                    "messages": [{"role": m["role"], "content": m["content"]} for m in user_msgs],
                    "max_tokens": kwargs.get("max_tokens", 2048)
                }
            
            elif provider == "cohere":
                headers["Authorization"] = f"Bearer {api_key}"
                url = f"{provider_config['base_url']}/chat"
                model = model or provider_config["models"][0]
                data = {
                    "model": model,
                    "message": messages[-1]["content"],
                    "chat_history": messages[:-1],
                    "temperature": kwargs.get("temperature", 0.7),
                    "max_tokens": kwargs.get("max_tokens", 2048)
                }
            
            response = requests.post(url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            result = response.json()
            
            # æå–å“åº”æ–‡æœ¬
            if provider == "anthropic":
                response_text = result["content"][0]["text"]
            elif provider == "cohere":
                response_text = result["text"]
            else:
                response_text = result["choices"][0]["message"]["content"]
            
            # æ›´æ–°ç»Ÿè®¡
            execution_time = time.time() - start_time
            stats = self.usage_stats[provider]
            stats["requests"] += 1
            stats["success"] += 1
            stats["avg_time"] = (stats["avg_time"] * (stats["requests"] - 1) + execution_time) / stats["requests"]
            self.health_status[provider]["failures"] = 0
            self.health_status[provider]["available"] = True
            
            # ç¼“å­˜ç»“æœ
            self.cache.set(response_text, cache_key)
            
            return {
                "content": response_text,
                "model": model,
                "provider": provider,
                "time": execution_time,
                "success": True
            }
        
        except Exception as e:
            # æ›´æ–°é”™è¯¯ç»Ÿè®¡
            execution_time = time.time() - start_time
            stats = self.usage_stats[provider]
            stats["requests"] += 1
            stats["errors"] += 1
            self.health_status[provider]["failures"] += 1
            if self.health_status[provider]["failures"] >= 3:
                self.health_status[provider]["available"] = False
            
            return {
                "error": str(e),
                "provider": provider,
                "success": False
            }

# ==================== çŸ¥è¯†åº“ç³»ç»Ÿ ====================

class AdvancedKnowledgeBase:
    """é«˜çº§çŸ¥è¯†åº“ç³»ç»Ÿ - å‘é‡æ£€ç´¢ + è¯­ä¹‰æœç´¢"""
    def __init__(self, cache: AsyncCache):
        self.cache = cache
        self.documents = {}
        self.document_vectors = None
        self.vectorizer = None
        self.neural_index = {}
        self.lock = threading.RLock()
    
    def add_document(self, doc_id: str, content: str, metadata: Dict = None):
        """æ·»åŠ æ–‡æ¡£"""
        with self.lock:
            self.documents[doc_id] = {
                "content": content,
                "metadata": metadata or {},
                "added_at": datetime.now(),
                "length": len(content),
                "tokens": len(content.split())
            }
            self._rebuild_index()
    
    def _rebuild_index(self):
        """é‡å»ºç´¢å¼•"""
        if not self.documents:
            return
        
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.metrics.pairwise import cosine_similarity
            
            texts = [doc["content"] for doc in self.documents.values()]
            self.vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))
            self.document_vectors = self.vectorizer.fit_transform(texts)
            
            # ç”Ÿæˆç¥ç»ç‰¹å¾
            for doc_id, doc in self.documents.items():
                self.neural_index[doc_id] = self._generate_neural_features(doc["content"])
        
        except Exception as e:
            print(f"Error rebuilding index: {e}")
    
    def _generate_neural_features(self, text: str) -> Dict[str, float]:
        """ç”Ÿæˆç¥ç»ç‰¹å¾"""
        return {
            "complexity": len(set(text.split())) / len(text.split()) if text.split() else 0,
            "density": text.count('ã€‚') / (len(text) / 100) if text else 0,
            "avg_sentence_length": np.mean([len(s.split()) for s in text.split('ã€‚') if s]) if text else 0
        }
    
    def search(self, query: str, top_k: int = 5, use_semantic: bool = True) -> List[Dict]:
        """æœç´¢æ–‡æ¡£"""
        cache_key = f"search_{hash(query)}_{top_k}_{use_semantic}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        if not self.documents or self.document_vectors is None:
            return []
        
        try:
            # è¯­ä¹‰æœç´¢
            if use_semantic and self.vectorizer:
                query_vector = self.vectorizer.transform([query])
                similarities = cosine_similarity(query_vector, self.document_vectors).flatten()
                
                top_indices = similarities.argsort()[-top_k:][::-1]
                results = []
                for idx in top_indices:
                    doc_id = list(self.documents.keys())[idx]
                    doc = self.documents[doc_id]
                    results.append({
                        "doc_id": doc_id,
                        "score": float(similarities[idx]),
                        "content": doc["content"][:500],
                        "metadata": doc["metadata"],
                        "neural_features": self.neural_index.get(doc_id, {})
                    })
                
                self.cache.set(results, cache_key)
                return results
        
        except Exception as e:
            print(f"Search error: {e}")
        
        return []
    
    def get_document_by_id(self, doc_id: str) -> Optional[Dict]:
        """æ ¹æ® ID è·å–æ–‡æ¡£"""
        return self.documents.get(doc_id)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                "total_documents": len(self.documents),
                "total_tokens": sum(doc["tokens"] for doc in self.documents.values()),
                "total_length": sum(doc["length"] for doc in self.documents.values()),
                "avg_document_length": np.mean([doc["length"] for doc in self.documents.values()]) if self.documents else 0
            }

# ==================== AI å‡ºé¢˜å¼•æ“ ====================

class AIQuestionGenerator:
    """AI æ™ºèƒ½å‡ºé¢˜å¼•æ“"""
    def __init__(self, api_manager: APIManager, knowledge_base: AdvancedKnowledgeBase):
        self.api_manager = api_manager
        self.knowledge_base = knowledge_base
        self.cache = AsyncCache()
        
        # é¢˜ç›®æ¨¡æ¿
        self.question_templates = {
            "reasoning": [
                "åŸºäºçŸ¥è¯†åº“ä¸­çš„{topic}ï¼Œè¯·åˆ†æ{concept1}ä¸{concept2}ä¹‹é—´çš„é€»è¾‘å…³ç³»",
                "å¦‚æœ{condition}æˆç«‹ï¼Œé‚£ä¹ˆ{result}ä¼šå‘ç”Ÿå˜åŒ–å—ï¼Ÿè¯·è¯´æ˜ç†ç”±",
                "è¯·ç”¨ä¸‰æ®µè®ºæ¨ç†åˆ†æä»¥ä¸‹é—®é¢˜ï¼š{problem}"
            ],
            "creative": [
                "è¯·ç”¨ä¸‰ç§ä¸åŒçš„åˆ›æ„æ–¹å¼æè¿°{concept}",
                "åŸºäº{context}ï¼Œè®¾è®¡ä¸€ä¸ªåˆ›æ–°çš„{idea}",
                "è¯·ä»åç›´è§‰çš„è§’åº¦æ€è€ƒ{topic}"
            ],
            "depth": [
                "ä»å¤šä¸ªå“²å­¦è§’åº¦æ·±åº¦æ€è€ƒï¼š{question}",
                "å¦‚æœ{hypothesis}æˆä¸ºç°å®ï¼Œè¿™å¯¹{domain}æ„å‘³ç€ä»€ä¹ˆï¼Ÿ",
                "è¯·åˆ†æ{concept}çš„æœ¬è´¨ï¼Œå¹¶æ¢è®¨å…¶æ·±å±‚æ„ä¹‰"
            ]
        }
    
    def generate_question(self, dimension: str, difficulty: int = 3, 
                          use_knowledge_base: bool = True) -> Optional[Question]:
        """ç”Ÿæˆé¢˜ç›®"""
        cache_key = f"q_{dimension}_{difficulty}_{use_knowledge_base}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        # å¦‚æœä½¿ç”¨çŸ¥è¯†åº“ï¼Œå…ˆæ£€ç´¢ç›¸å…³å†…å®¹
        context = ""
        reference = ""
        if use_knowledge_base and self.knowledge_base.documents:
            search_results = self.knowledge_base.search(dimension, top_k=1)
            if search_results:
                context = search_results[0]["content"][:300]
                reference = search_results[0]["metadata"].get("filename", "")
        
        # æ„å»º prompt
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ„è¯†æµ‹è¯•å‡ºé¢˜ä¸“å®¶ã€‚è¯·ä¸º"{dimension}"ç»´åº¦ç”Ÿæˆä¸€ä¸ªéš¾åº¦ä¸º{difficulty}çº§ï¼ˆ1-5çº§ï¼‰çš„æµ‹è¯•é¢˜ç›®ã€‚
        
        {"ä»¥ä¸‹æ˜¯ç›¸å…³çš„çŸ¥è¯†åº“å†…å®¹ï¼Œè¯·åŸºäºæ­¤ç”Ÿæˆé¢˜ç›®ï¼š\n" + context if context else "è¯·è‡ªè¡Œè®¾è®¡ä¸€ä¸ªæœ‰æ·±åº¦çš„é¢˜ç›®"}
        
        è¦æ±‚ï¼š
        1. é¢˜ç›®å…·æœ‰å¯å‘æ€§å’ŒæŒ‘æˆ˜æ€§
        2. èƒ½å¤ŸçœŸæ­£æµ‹è¯•ç”¨æˆ·çš„{dimension}èƒ½åŠ›
        3. å¼€æ”¾æ€§é—®é¢˜ï¼Œé¼“åŠ±æ·±å…¥æ€è€ƒ
        4. é¢˜ç›®ç®€æ´æ˜äº†
        
        è¯·ç›´æ¥è¾“å‡ºé¢˜ç›®å†…å®¹ï¼Œä¸éœ€è¦å…¶ä»–è§£é‡Šã€‚
        """
        
        # è°ƒç”¨ API ç”Ÿæˆ
        provider = self.api_manager.select_best_provider(task_type="quality")
        if not provider:
            return None
        
        response = self.api_manager.call_api(
            provider=provider,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ„è¯†æµ‹è¯•å‡ºé¢˜ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ]
        )
        
        if not response.get("success"):
            return None
        
        question_content = response["content"]
        
        # åˆ›å»ºé¢˜ç›®å¯¹è±¡
        question = Question(
            id=hashlib.md5(question_content.encode()).hexdigest(),
            dimension=dimension,
            difficulty=QuestionDifficulty(min(5, max(1, difficulty))),
            content=question_content,
            type="open",
            reference=reference,
            metadata={
                "generated_by": "ai",
                "provider": provider,
                "model": response["model"],
                "difficulty": difficulty,
                "timestamp": datetime.now().isoformat()
            }
        )
        
        self.cache.set(question, cache_key)
        return question
    
    def generate_batch_questions(self, dimensions: List[str], 
                                count_per_dimension: int = 1) -> List[Question]:
        """æ‰¹é‡ç”Ÿæˆé¢˜ç›®"""
        questions = []
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            for dimension in dimensions:
                for _ in range(count_per_dimension):
                    future = executor.submit(
                        self.generate_question, 
                        dimension, 
                        difficulty=random.randint(2, 4)
                    )
                    futures.append(future)
            
            for future in as_completed(futures):
                question = future.result()
                if question:
                    questions.append(question)
        
        return questions

# ==================== AI è¯„åˆ†å¼•æ“ ====================

class AIGradingEngine:
    """AI æ·±åº¦è¯„åˆ†å¼•æ“"""
    def __init__(self, api_manager: APIManager, knowledge_base: AdvancedKnowledgeBase):
        self.api_manager = api_manager
        self.knowledge_base = knowledge_base
        self.cache = AsyncCache()
        
        # è¯„åˆ†ç»´åº¦
        self.grading_dimensions = {
            "logic": {"weight": 0.25, "desc": "é€»è¾‘æ€§"},
            "creativity": {"weight": 0.20, "desc": "åˆ›é€ æ€§"},
            "depth": {"weight": 0.25, "desc": "æ·±åº¦"},
            "accuracy": {"weight": 0.15, "desc": "å‡†ç¡®æ€§"},
            "completeness": {"weight": 0.15, "desc": "å®Œæ•´æ€§"}
        }
    
    def grade_answer(self, question: Question, answer: str) -> Dict[str, Any]:
        """è¯„åˆ†ç­”æ¡ˆ"""
        cache_key = f"grade_{question.id}_{hash(answer)}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        # æ£€ç´¢ç›¸å…³çŸ¥è¯†åº“å†…å®¹
        context = ""
        if self.knowledge_base.documents:
            search_results = self.knowledge_base.search(question.dimension, top_k=2)
            if search_results:
                context = "\n".join([r["content"][:200] for r in search_results])
        
        # æ„å»º prompt
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ„è¯†æµ‹è¯•è¯„åˆ†ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹ç­”æ¡ˆè¿›è¡Œæ·±åº¦è¯„åˆ†ã€‚
        
        === æµ‹è¯•ç»´åº¦ ===
        {question.dimension}
        
        === é¢˜ç›® ===
        {question.content}
        
        === å‚è€ƒå†…å®¹ ===
        {context if context else "ï¼ˆæ— ï¼‰"}
        
        === ç”¨æˆ·ç­”æ¡ˆ ===
        {answer}
        
        è¯„åˆ†ç»´åº¦ï¼š
        1. é€»è¾‘æ€§ (0-10åˆ†)ï¼šæ¨ç†æ˜¯å¦ä¸¥å¯†ï¼Œé€»è¾‘æ˜¯å¦æ¸…æ™°
        2. åˆ›é€ æ€§ (0-10åˆ†)ï¼šæ˜¯å¦æœ‰æ–°é¢–è§è§£ï¼Œæ˜¯å¦æ‰“ç ´å¸¸è§„æ€ç»´
        3. æ·±åº¦ (0-10åˆ†)ï¼šæ€è€ƒæ˜¯å¦æ·±å…¥ï¼Œæ˜¯å¦è§¦åŠæœ¬è´¨
        4. å‡†ç¡®æ€§ (0-10åˆ†)ï¼šä¸å‚è€ƒå†…å®¹çš„åŒ¹é…åº¦ï¼Œäº‹å®æ˜¯å¦å‡†ç¡®
        5. å®Œæ•´æ€§ (0-10åˆ†)ï¼šå›ç­”æ˜¯å¦å…¨é¢ï¼Œæ˜¯å¦æœ‰é—æ¼
        
        è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºè¯„åˆ†ç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        {{
            "logic": åˆ†æ•°,
            "creativity": åˆ†æ•°,
            "depth": åˆ†æ•°,
            "accuracy": åˆ†æ•°,
            "completeness": åˆ†æ•°,
            "total_score": åŠ æƒæ€»åˆ†,
            "feedback": "è¯¦ç»†åé¦ˆ",
            "strengths": ["ä¼˜åŠ¿1", "ä¼˜åŠ¿2"],
            "weaknesses": ["æ”¹è¿›ç‚¹1", "æ”¹è¿›ç‚¹2"],
            "confidence": è¯„åˆ†ç½®ä¿¡åº¦(0-1)
        }}
        
        è¯·åªè¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚
        """
        
        # è°ƒç”¨ API è¯„åˆ†
        provider = self.api_manager.select_best_provider(task_type="quality")
        if not provider:
            return self._default_grading()
        
        response = self.api_manager.call_api(
            provider=provider,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ„è¯†æµ‹è¯•è¯„åˆ†ä¸“å®¶ï¼Œå¿…é¡»ä»¥ JSON æ ¼å¼è¾“å‡ºè¯„åˆ†ç»“æœã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3  # é™ä½éšæœºæ€§
        )
        
        if not response.get("success"):
            return self._default_grading()
        
        try:
            # æå– JSON
            content = response["content"]
            # å°è¯•æå– JSON éƒ¨åˆ†
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                grading_result = json.loads(json_match.group())
                
                # éªŒè¯å­—æ®µ
                required_fields = ["logic", "creativity", "depth", "accuracy", "completeness", 
                                  "total_score", "feedback", "confidence"]
                if all(field in grading_result for field in required_fields):
                    grading_result["provider"] = provider
                    grading_result["model"] = response["model"]
                    self.cache.set(grading_result, cache_key)
                    return grading_result
        
        except Exception as e:
            print(f"Grading error: {e}")
        
        return self._default_grading()
    
    def _default_grading(self) -> Dict[str, Any]:
        """é»˜è®¤è¯„åˆ†ï¼ˆå¤±è´¥æ—¶ï¼‰"""
        return {
            "logic": random.uniform(5, 8),
            "creativity": random.uniform(5, 8),
            "depth": random.uniform(5, 8),
            "accuracy": random.uniform(5, 8),
            "completeness": random.uniform(5, 8),
            "total_score": random.uniform(6, 9),
            "feedback": "ç³»ç»Ÿè¯„åˆ†ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†",
            "strengths": ["å®Œæ•´å›ç­”"],
            "weaknesses": ["éœ€è¦æ›´æ·±å…¥çš„åˆ†æ"],
            "confidence": 0.5,
            "provider": "default"
        }

# ==================== å¤šæ¨¡å‹èåˆå¯¹è¯ç³»ç»Ÿ ====================

class MultiModelDialogueSystem:
    """å¤šæ¨¡å‹èåˆå¯¹è¯ç³»ç»Ÿ"""
    def __init__(self, api_manager: APIManager, cache: AsyncCache):
        self.api_manager = api_manager
        self.cache = cache
        self.conversation_history = deque(maxlen=100)
    
    def dialogue(self, user_input: str, context: str = "", 
                  complexity: int = 5, use_ensemble: bool = True) -> Dict[str, Any]:
        """å¯¹è¯"""
        cache_key = f"dialog_{hash(user_input)}_{complexity}_{use_ensemble}"
        cached = self.cache.get(cache_key)
        if cached:
            return cached
        
        # æ„å»ºæ¶ˆæ¯
        messages = []
        if context:
            messages.append({"role": "system", "content": f"èƒŒæ™¯ï¼š{context}"})
        
        # æ·»åŠ å†å²å¯¹è¯ï¼ˆæœ€è¿‘ 5 æ¡ï¼‰
        for msg in list(self.conversation_history)[-5:]:
            messages.append({"role": msg["role"], "content": msg["content"]})
        
        messages.append({"role": "user", "content": user_input})
        
        if use_ensemble:
            # å¤šæ¨¡å‹èåˆ
            return self._ensemble_dialogue(messages, complexity)
        else:
            # å•æ¨¡å‹
            provider = self.api_manager.select_best_provider(task_type="quality")
            if not provider:
                return {"error": "No available API provider"}
            
            response = self.api_manager.call_api(provider, messages, temperature=0.7)
            if response.get("success"):
                return {
                    "content": response["content"],
                    "provider": provider,
                    "model": response["model"],
                    "confidence": 0.8,
                    "models_used": [provider],
                    "time": response["time"]
                }
            else:
                return {"error": response["error"]}
    
    def _ensemble_dialogue(self, messages: List[Dict], complexity: int) -> Dict[str, Any]:
        """å¤šæ¨¡å‹èåˆå¯¹è¯"""
        providers = self.api_manager.get_available_providers()
        if not providers:
            return {"error": "No available API providers"}
        
        # å¹¶å‘è°ƒç”¨å¤šä¸ª API
        results = []
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            for provider in providers[:3]:  # æœ€å¤šä½¿ç”¨ 3 ä¸ª API
                future = executor.submit(
                    self.api_manager.call_api,
                    provider,
                    messages,
                    temperature=0.7
                )
                futures.append(future)
            
            for future in as_completed(futures):
                result = future.result()
                if result.get("success"):
                    results.append(result)
        
        if not results:
            return {"error": "All API calls failed"}
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªç»“æœï¼Œç›´æ¥è¿”å›
        if len(results) == 1:
            return {
                "content": results[0]["content"],
                "provider": results[0]["provider"],
                "model": results[0]["model"],
                "confidence": 0.8,
                "models_used": [results[0]["provider"]],
                "time": results[0]["time"]
            }
        
        # å¤šç»“æœèåˆ
        return self._synthesize_results(results, messages)
    
    def _synthesize_results(self, results: List[Dict], messages: List[Dict]) -> Dict[str, Any]:
        """èåˆå¤šä¸ªç»“æœ"""
        # æ„å»ºèåˆ prompt
        results_text = "\n\n".join([
            f"=== {r['provider']} ({r['model']}) ===\n{r['content']}"
            for r in results
        ])
        
        synthesis_prompt = f"""
        ä½ æ˜¯ç¥ç»ç¬¦å·èåˆä¸“å®¶ã€‚è¯·èåˆä»¥ä¸‹å¤šä¸ª AI æ¨¡å‹çš„å›ç­”ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
        
        {results_text}
        
        === åŸå§‹é—®é¢˜ ===
        {messages[-1]['content']}
        
        è¦æ±‚ï¼š
        1. ç»¼åˆå„æ¨¡å‹çš„ä¼˜ç‚¹ï¼Œç”Ÿæˆä¸€ä¸ªæ›´å…¨é¢ã€å‡†ç¡®çš„ç­”æ¡ˆ
        2. æŒ‡å‡ºå„æ¨¡å‹å›ç­”çš„äº®ç‚¹å’Œä¸è¶³
        3. ç»™å‡ºæœ€ç»ˆçš„èåˆç­”æ¡ˆ
        4. è¯„ä¼°èåˆç­”æ¡ˆçš„ç½®ä¿¡åº¦ (0-1)
        
        è¯·ä»¥ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
        {{
            "content": "æœ€ç»ˆèåˆç­”æ¡ˆ",
            "highlights": ["æ¨¡å‹1çš„ä¼˜ç‚¹", "æ¨¡å‹2çš„ä¼˜ç‚¹"],
            "critique": "å¯¹å„æ¨¡å‹çš„æ‰¹è¯„",
            "confidence": ç½®ä¿¡åº¦
        }}
        """
        
        provider = self.api_manager.select_best_provider(task_type="quality")
        if not provider:
            # ç®€å•ç­–ç•¥ï¼šè¿”å›æœ€é•¿çš„ç­”æ¡ˆ
            best_result = max(results, key=lambda r: len(r["content"]))
            return {
                "content": best_result["content"],
                "provider": best_result["provider"],
                "model": best_result["model"],
                "confidence": 0.7,
                "models_used": [r["provider"] for r in results],
                "time": sum(r["time"] for r in results)
            }
        
        response = self.api_manager.call_api(
            provider,
            [
                {"role": "system", "content": "ä½ æ˜¯ç¥ç»ç¬¦å·èåˆä¸“å®¶ï¼Œå¿…é¡»ä»¥ JSON æ ¼å¼è¾“å‡ºã€‚"},
                {"role": "user", "content": synthesis_prompt}
            ],
            temperature=0.5
        )
        
        if response.get("success"):
            try:
                json_match = re.search(r'\{.*\}', response["content"], re.DOTALL)
                if json_match:
                    synthesis_result = json.loads(json_match.group())
                    return {
                        "content": synthesis_result.get("content", response["content"]),
                        "provider": provider,
                        "model": response["model"],
                        "confidence": synthesis_result.get("confidence", 0.8),
                        "models_used": [r["provider"] for r in results],
                        "time": sum(r["time"] for r in results) + response["time"],
                        "synthesis_details": synthesis_result
                    }
            except:
                pass
        
        # å›é€€åˆ°æœ€é•¿ç­”æ¡ˆ
        best_result = max(results, key=lambda r: len(r["content"]))
        return {
            "content": best_result["content"],
            "provider": best_result["provider"],
            "model": best_result["model"],
            "confidence": 0.7,
            "models_used": [r["provider"] for r in results],
            "time": sum(r["time"] for r in results)
        }

# ==================== è¿›åŒ–é¢„æµ‹ç³»ç»Ÿ ====================

class EvolutionPredictor:
    """è¿›åŒ–é¢„æµ‹ç³»ç»Ÿ - æœºå™¨å­¦ä¹ å¢å¼º"""
    def __init__(self, cache: AsyncCache):
        self.cache = cache
        self.history = deque(maxlen=1000)
        self.predictions = deque(maxlen=100)
    
    def add_snapshot(self, snapshot: ConsciousnessSnapshot):
        """æ·»åŠ å¿«ç…§"""
        self.history.append(snapshot)
    
    def predict(self, horizon: int = 5, method: str = "auto") -> Dict[str, Any]:
        """é¢„æµ‹è¿›åŒ–"""
        if len(self.history) < 3:
            return {"error": "å†å²æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦ 3 æ¬¡æµ‹è¯•è®°å½•"}
        
        try:
            snapshots = list(self.history)
            dimensions = self.history[0].scores.keys()
            
            predictions = {}
            for dim in dimensions:
                # æå–æ—¶é—´åºåˆ—
                values = [s.scores[dim] for s in snapshots]
                
                # å¤šç§é¢„æµ‹æ–¹æ³•
                methods_result = {}
                
                # 1. çº¿æ€§å›å½’
                lr_pred = self._linear_regression_prediction(values, horizon)
                methods_result["linear_regression"] = lr_pred
                
                # 2. ç§»åŠ¨å¹³å‡
                ma_pred = self._moving_average_prediction(values, horizon)
                methods_result["moving_average"] = ma_pred
                
                # 3. æŒ‡æ•°å¹³æ»‘
                es_pred = self._exponential_smoothing_prediction(values, horizon)
                methods_result["exponential_smoothing"] = es_pred
                
                # 4. è‡ªé€‚åº”æ–¹æ³•ï¼ˆæ ¹æ®å†å²é€‰æ‹©æœ€ä½³ï¼‰
                if method == "auto":
                    best_method = self._select_best_method(dim, values, methods_result)
                else:
                    best_method = method
                
                predictions[dim] = {
                    "current": values[-1],
                    "predicted": methods_result[best_method][-1],
                    "trajectory": methods_result[best_method],
                    "method_used": best_method,
                    "trend": "ä¸Šå‡" if methods_result[best_method][-1] > values[-1] else "ä¸‹é™",
                    "confidence": min(0.95, len(self.history) / 100)
                }
            
            # æ•´ä½“è¿›åŒ–è¶‹åŠ¿
            total_current = sum(p["current"] for p in predictions.values())
            total_predicted = sum(p["predicted"] for p in predictions.values())
            overall_trend = "å¿«é€Ÿè¿›åŒ–æœŸ" if (total_predicted - total_current) > 5 else \
                           "ç¨³æ­¥æå‡ä¸­" if (total_predicted - total_current) > 0 else "å¹³ç¨³è¿‡æ¸¡æœŸ"
            
            return {
                "predictions": predictions,
                "overall_trend": overall_trend,
                "total_current": total_current,
                "total_predicted": total_predicted,
                "recommendations": self._generate_recommendations(predictions)
            }
        
        except Exception as e:
            return {"error": f"é¢„æµ‹å¤±è´¥: {str(e)}"}
    
    def _linear_regression_prediction(self, values: List[float], horizon: int) -> List[float]:
        """çº¿æ€§å›å½’é¢„æµ‹"""
        x = np.arange(len(values))
        z = np.polyfit(x, values, 1)
        p = np.poly1d(z)
        future_x = np.arange(len(values), len(values) + horizon)
        future_values = p(future_x)
        return np.clip(future_values, 0, 14).tolist()
    
    def _moving_average_prediction(self, values: List[float], horizon: int) -> List[float]:
        """ç§»åŠ¨å¹³å‡é¢„æµ‹"""
        if len(values) < 2:
            return [values[-1]] * horizon
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡æ–œç‡
        ma_slope = (values[-1] - values[-2]) if len(values) >= 2 else 0
        predictions = []
        for i in range(horizon):
            next_value = values[-1] + ma_slope * (i + 1)
            predictions.append(np.clip(next_value, 0, 14))
        return predictions
    
    def _exponential_smoothing_prediction(self, values: List[float], horizon: int) -> List[float]:
        """æŒ‡æ•°å¹³æ»‘é¢„æµ‹"""
        alpha = 0.3
        smoothed = values[0]
        for value in values[1:]:
            smoothed = alpha * value + (1 - alpha) * smoothed
        
        # ä½¿ç”¨æœ€åçš„å¹³æ»‘å€¼å’Œè¶‹åŠ¿
        trend = (values[-1] - values[-2]) if len(values) >= 2 else 0
        predictions = []
        for i in range(horizon):
            next_value = smoothed + trend * (i + 1)
            predictions.append(np.clip(next_value, 0, 14))
        return predictions
    
    def _select_best_method(self, dim: str, values: List[float], 
                            methods: Dict[str, List[float]]) -> str:
        """é€‰æ‹©æœ€ä½³é¢„æµ‹æ–¹æ³•"""
        # ç®€å•ç­–ç•¥ï¼šé€‰æ‹©æ–¹å·®æœ€å°çš„æ–¹æ³•
        variances = {}
        for method, predictions in methods.items():
            variances[method] = np.var(predictions)
        
        return min(variances.keys(), key=lambda k: variances[k])
    
    def _generate_recommendations(self, predictions: Dict) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        for dim, pred in predictions.items():
            if pred["predicted"] < pred["current"]:
                recommendations.append(
                    f"{dim}: å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œå»ºè®®åŠ å¼ºé’ˆå¯¹æ€§è®­ç»ƒ"
                )
            elif pred["predicted"] - pred["current"] > 1.5:
                recommendations.append(
                    f"{dim}: è¿›åŒ–è¶‹åŠ¿è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ"
                )
        
        return recommendations

# ==================== æ¸¸æˆåŒ–ç³»ç»Ÿ ====================

class GamificationSystem:
    """æ¸¸æˆåŒ–ç³»ç»Ÿ"""
    def __init__(self, config: SystemConfig):
        self.config = config
        self.user_profile = {
            "level": 1,
            "exp": 0,
            "badges": [],
            "streak": 0,
            "last_active": datetime.now()
        }
    
    def add_exp(self, amount: int, reason: str = "") -> Dict[str, Any]:
        """å¢åŠ ç»éªŒå€¼"""
        self.user_profile["exp"] += amount
        self.user_profile["last_active"] = datetime.now()
        
        # æ£€æŸ¥å‡çº§
        old_level = self.user_profile["level"]
        self.user_profile["level"] = self._calculate_level()
        new_level = self.user_profile["level"]
        
        level_up = new_level > old_level
        level_rewards = []
        
        if level_up:
            for level in range(old_level + 1, new_level + 1):
                reward = self._get_level_reward(level)
                if reward:
                    level_rewards.append(reward)
        
        return {
            "exp_gained": amount,
            "total_exp": self.user_profile["exp"],
            "old_level": old_level,
            "new_level": new_level,
            "level_up": level_up,
            "rewards": level_rewards,
            "reason": reason
        }
    
    def _calculate_level(self) -> int:
        """è®¡ç®—ç­‰çº§"""
        exp = self.user_profile["exp"]
        base = self.config.level_config["exp_base"]
        growth = self.config.level_config["exp_growth"]
        max_level = self.config.level_config["max_level"]
        
        # ä½¿ç”¨å¯¹æ•°å¢é•¿å…¬å¼
        level = int(np.log(exp / base + 1) / np.log(growth)) + 1
        return min(level, max_level)
    
    def _get_level_reward(self, level: int) -> Optional[str]:
        """è·å–ç­‰çº§å¥–åŠ±"""
        rewards = {
            5: "è§£é”ï¼šè‡ªå®šä¹‰é¢˜ç›®",
            10: "è§£é”ï¼šå¤šæ¨¡å‹èåˆå¯¹è¯",
            20: "è§£é”ï¼šé«˜çº§é¢„æµ‹æ¨¡å¼",
            30: "è§£é”ï¼šçŸ¥è¯†å›¾è°±å¯è§†åŒ–",
            50: "è§£é”ï¼šå®Œæ•´ç³»ç»Ÿæƒé™",
            100: "è§£é”ï¼šæ„è¯†å¤§å¸ˆç§°å·"
        }
        return rewards.get(level)
    
    def check_achievements(self, snapshot: ConsciousnessSnapshot, 
                           test_count: int, doc_count: int, 
                           conversation_count: int) -> List[Dict[str, Any]]:
        """æ£€æŸ¥æˆå°±"""
        new_achievements = []
        
        # æ£€æŸ¥å„ç§æˆå°±æ¡ä»¶
        if test_count >= 1 and "first_test" not in [a["id"] for a in self.user_profile["badges"]]:
            new_achievements.append(self._unlock_achievement("first_test"))
        
        if test_count >= 10 and "test_master" not in [a["id"] for a in self.user_profile["badges"]]:
            new_achievements.append(self._unlock_achievement("test_master"))
        
        if snapshot.total_score >= 80 and "perfect_score" not in [a["id"] for a in self.user_profile["badges"]]:
            new_achievements.append(self._unlock_achievement("perfect_score"))
        
        if self.user_profile["level"] >= 10 and "level_10" not in [a["id"] for a in self.user_profile["badges"]]:
            new_achievements.append(self._unlock_achievement("level_10"))
        
        if doc_count >= 10 and "knowledge_collector" not in [a["id"] for a in self.user_profile["badges"]]:
            new_achievements.append(self._unlock_achievement("knowledge_collector"))
        
        if conversation_count >= 100 and "conversation_100" not in [a["id"] for a in self.user_profile["badges"]]:
            new_achievements.append(self._unlock_achievement("conversation_100"))
        
        # æ£€æŸ¥å…¨èƒ½å¤§å¸ˆ
        all_high = all(score >= 8 for score in snapshot.scores.values())
        if all_high and "all_dimensions" not in [a["id"] for a in self.user_profile["badges"]]:
            new_achievements.append(self._unlock_achievement("all_dimensions"))
        
        return new_achievements
    
    def _unlock_achievement(self, achievement_id: str) -> Dict[str, Any]:
        """è§£é”æˆå°±"""
        achievement_config = self.config.achievements_config[achievement_id]
        achievement = {
            "id": achievement_id,
            "name": achievement_config["name"],
            "desc": achievement_config["desc"],
            "exp": achievement_config["exp"],
            "unlocked_at": datetime.now().isoformat()
        }
        self.user_profile["badges"].append(achievement)
        self.add_exp(achievement_config["exp"], f"æˆå°±è§£é”: {achievement['name']}")
        return achievement

# ==================== å¯è§†åŒ–ç³»ç»Ÿ ====================

class AdvancedVisualizer:
    """é«˜çº§å¯è§†åŒ–ç³»ç»Ÿ"""
    def __init__(self):
        self.colors = {
            "primary": "#6366F1",
            "secondary": "#8B5CF6",
            "accent": "#EC4899",
            "success": "#10B981",
            "warning": "#F59E0B",
            "error": "#EF4444"
        }
    
    def generate_radar_chart(self, scores: Dict[str, float], 
                             history: List[Dict] = None) -> str:
        """ç”Ÿæˆé›·è¾¾å›¾"""
        try:
            dimensions = list(scores.keys())
            values = list(scores.values())
            values += values[:1]
            
            fig = plt.figure(figsize=(12, 10))
            ax = fig.add_subplot(111, polar=True)
            
            angles = np.linspace(0, 2*np.pi, len(dimensions), endpoint=False).tolist()
            angles += angles[:1]
            
            # å½“å‰çŠ¶æ€
            ax.plot(angles, values, 'o-', linewidth=3, 
                   color=self.colors["primary"], 
                   label='å½“å‰çŠ¶æ€', markersize=8)
            ax.fill(angles, values, alpha=0.3, color=self.colors["primary"])
            
            # å†å²è½¨è¿¹
            if history and len(history) >= 2:
                colors = [self.colors["secondary"], self.colors["accent"], "#60A5FA"]
                for i, hist in enumerate(history[-3:]):
                    hist_values = [hist.get(dim, 0) for dim in dimensions]
                    hist_values += hist_values[:1]
                    alpha = 0.15 + (i / 3) * 0.25
                    ax.plot(angles, hist_values, '-', linewidth=2, 
                           alpha=alpha, color=colors[i % len(colors)],
                           label=f'å†å²{i+1}')
            
            ax.set_thetagrids(np.degrees(angles[:-1]), dimensions, fontsize=11, fontweight='bold')
            ax.set_ylim(0, 14)
            ax.set_title('æ„è¯†å¼ºåº¦é›·è¾¾å›¾', fontsize=18, pad=25, fontweight='bold')
            ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)
            ax.grid(True, alpha=0.3)
            
            buf = io.BytesIO()
            plt.savefig(buf, format='png', bbox_inches='tight', dpi=200, facecolor='#0F0F1A')
            buf.seek(0)
            img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
            plt.close()
            
            return f"data:image/png;base64,{img_base64}"
        
        except Exception as e:
            print(f"Radar chart error: {e}")
            return None
    
    def generate_evolution_chart(self, history: List[Dict]) -> str:
        """ç”Ÿæˆè¿›åŒ–è¶‹åŠ¿å›¾"""
        try:
            if len(history) < 2:
                return None
            
            dimensions = list(history[0].get('category_scores', {}).keys())
            timestamps = [datetime.fromisoformat(h['timestamp']) for h in history]
            
            fig, axes = plt.subplots(3, 4, figsize=(16, 12))
            fig.suptitle('æ„è¯†è¿›åŒ–è¶‹åŠ¿', fontsize=18, fontweight='bold')
            
            for idx, dim in enumerate(dimensions):
                row = idx // 4
                col = idx % 4
                ax = axes[row, col]
                
                values = [h.get('category_scores', {}).get(dim, 0) for h in history]
                ax.plot(timestamps, values, marker='o', linewidth=2, 
                       color=self.colors["primary"], markersize=6)
                ax.fill_between(timestamps, values, alpha=0.2, color=self.colors["primary"])
                ax.set_title(dim, fontsize=10, fontweight='bold')
                ax.grid(True, alpha=0.3)
                ax.set_ylim(0, 14)
                
                # æ—‹è½¬ x è½´æ ‡ç­¾
                plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
            
            plt.tight_layout()
            
            buf = io.BytesIO()
            plt.savefig(buf, format='png', bbox_inches='tight', dpi=150, facecolor='#0F0F1A')
            buf.seek(0)
            img_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
            plt.close()
            
            return f"data:image/png;base64,{img_base64}"
        
        except Exception as e:
            print(f"Evolution chart error: {e}")
            return None

# ==================== æ•°æ®ç®¡ç† ====================

class DataManager:
    """æ•°æ®ç®¡ç†å™¨"""
    def __init__(self):
        self.test_results = []
        self.conversation_history = []
        self.user_data = {}
    
    def save_test_result(self, result: TestResult):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        self.test_results.append({
            "timestamp": result.snapshot.timestamp.isoformat(),
            "scores": result.snapshot.scores,
            "total_score": result.snapshot.total_score,
            "level": result.snapshot.level.value,
            "question_count": result.question_count,
            "accuracy": result.accuracy
        })
    
    def save_conversation(self, message: ConversationMessage):
        """ä¿å­˜å¯¹è¯"""
        self.conversation_history.append({
            "role": message.role,
            "content": message.content,
            "timestamp": message.timestamp.isoformat(),
            "confidence": message.confidence
        })
    
    def export_data(self) -> Dict[str, Any]:
        """å¯¼å‡ºæ•°æ®"""
        return {
            "export_time": datetime.now().isoformat(),
            "test_results": self.test_results,
            "conversation_history": self.conversation_history[-50:],  # æœ€è¿‘ 50 æ¡
            "user_data": self.user_data
        }
    
    def import_data(self, data: Dict[str, Any]):
        """å¯¼å…¥æ•°æ®"""
        if "test_results" in data:
            self.test_results.extend(data["test_results"])
        if "conversation_history" in data:
            self.conversation_history.extend(data["conversation_history"])
        if "user_data" in data:
            self.user_data.update(data["user_data"])

# ==================== ä¸»ç³»ç»Ÿ ====================

class MultiverseSystem:
    """å¤šç»´è½®å›ç ´è§£ç³»ç»Ÿä¸»ç±»"""
    def __init__(self):
        self.config = SystemConfig()
        self.api_keys = self._load_api_keys()
        self.cache = AsyncCache()
        
        # åˆå§‹åŒ–å­ç³»ç»Ÿ
        self.api_manager = APIManager(self.api_keys, self.config)
        self.knowledge_base = AdvancedKnowledgeBase(self.cache)
        self.question_generator = AIQuestionGenerator(self.api_manager, self.knowledge_base)
        self.grading_engine = AIGradingEngine(self.api_manager, self.knowledge_base)
        self.dialogue_system = MultiModelDialogueSystem(self.api_manager, self.cache)
        self.predictor = EvolutionPredictor(self.cache)
        self.gamification = GamificationSystem(self.config)
        self.visualizer = AdvancedVisualizer()
        self.data_manager = DataManager()
    
    def _load_api_keys(self) -> Dict[str, str]:
        """åŠ è½½ API å¯†é’¥"""
        keys = {}
        
        # ä» Streamlit Secrets åŠ è½½
        if hasattr(st, 'secrets'):
            keys['groq'] = st.secrets.get('GROQ_API_KEY', '')
            keys['openai'] = st.secrets.get('OPENAI_API_KEY', '')
            keys['anthropic'] = st.secrets.get('ANTHROPIC_API_KEY', '')
            keys['cohere'] = st.secrets.get('COHERE_API_KEY', '')
        
        # ä»ç¯å¢ƒå˜é‡åŠ è½½
        if not keys.get('groq'):
            keys['groq'] = os.environ.get('GROQ_API_KEY', '')
        if not keys.get('openai'):
            keys['openai'] = os.environ.get('OPENAI_API_KEY', '')
        if not keys.get('anthropic'):
            keys['anthropic'] = os.environ.get('ANTHROPIC_API_KEY', '')
        if not keys.get('cohere'):
            keys['cohere'] = os.environ.get('COHERE_API_KEY', '')
        
        return keys
    
    def get_api_status(self) -> Dict[str, Any]:
        """è·å– API çŠ¶æ€"""
        status = {}
        for provider, config in self.config.api_providers.items():
            has_key = bool(self.api_keys.get(provider))
            health = self.api_manager.health_status[provider]
            stats = self.api_manager.usage_stats[provider]
            
            status[provider] = {
                "name": config["name"],
                "configured": has_key,
                "available": health["available"],
                "free": config["free"],
                "requests": stats["requests"],
                "success_rate": stats["success"] / stats["requests"] if stats["requests"] > 0 else 0,
                "avg_time": f"{stats['avg_time']:.2f}s"
            }
        
        return status

# ==================== Streamlit ç•Œé¢ ====================

def initialize_session_state():
    """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
    if 'system' not in st.session_state:
        st.session_state.system = MultiverseSystem()
    
    if 'test_in_progress' not in st.session_state:
        st.session_state.test_in_progress = False
    
    if 'current_questions' not in st.session_state:
        st.session_state.current_questions = []
    
    if 'current_question_index' not in st.session_state:
        st.session_state.current_question_index = 0
    
    if 'test_answers' not in st.session_state:
        st.session_state.test_answers = []

def show_custom_css():
    """æ˜¾ç¤ºè‡ªå®šä¹‰ CSS"""
    st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
    
    body {
        background: linear-gradient(135deg, #0F0F1A 0%, #1A1A2E 50%, #16213E 100%);
        color: #F1F5F9;
        font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
    }
    
    .stApp {
        background: transparent;
    }
    
    .title-gradient {
        background: linear-gradient(135deg, #6366F1 0%, #8B5CF6 50%, #EC4899 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
    }
    
    .glass-card {
        background: rgba(255, 255, 255, 0.05);
        border: 1px solid rgba(255, 255, 255, 0.1);
        border-radius: 16px;
        padding: 20px;
        backdrop-filter: blur(10px);
        box-shadow: 0 4px 16px rgba(0, 0, 0, 0.1);
        transition: all 0.3s ease;
    }
    .glass-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 24px rgba(0, 0, 0, 0.15);
    }
    
    .stat-card {
        background: linear-gradient(135deg, rgba(99, 102, 241, 0.1) 0%, rgba(139, 92, 246, 0.1) 100%);
        border: 1px solid rgba(99, 102, 241, 0.2);
        border-radius: 12px;
        padding: 16px;
        text-align: center;
    }
    
    .progress-container {
        background: rgba(255, 255, 255, 0.1);
        border-radius: 10px;
        overflow: hidden;
    }
    .progress-bar {
        background: linear-gradient(90deg, #6366F1 0%, #8B5CF6 100%);
        height: 10px;
        transition: width 0.5s ease;
    }
    
    .badge {
        display: inline-block;
        padding: 6px 14px;
        border-radius: 20px;
        font-size: 0.85rem;
        margin: 4px;
        background: linear-gradient(135deg, #6366F1 0%, #8B5CF6 100%);
        box-shadow: 0 2px 8px rgba(99, 102, 241, 0.3);
    }
    
    .stButton>button {
        background: linear-gradient(135deg, #6366F1 0%, #8B5CF6 100%);
        color: white;
        border: none;
        border-radius: 10px;
        padding: 12px 24px;
        font-weight: 600;
        transition: all 0.3s ease;
    }
    .stButton>button:hover {
        transform: scale(1.02);
        box-shadow: 0 4px 20px rgba(99, 102, 241, 0.4);
    }
    
    @keyframes pulse {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.5; }
    }
    .animate-pulse {
        animation: pulse 2s ease-in-out infinite;
    }
    
    @media (max-width: 768px) {
        h1 { font-size: 1.8em !important; }
        h2 { font-size: 1.5em !important; }
        .glass-card { padding: 16px; }
    }
    </style>
    """, unsafe_allow_html=True)

def show_header():
    """æ˜¾ç¤ºæ ‡é¢˜"""
    st.markdown("""
    <div style="text-align: center; padding: 30px 0;">
        <h1 class="title-gradient" style="font-size: 2.5em; margin: 0; font-weight: 700;">
            å¤šç»´è½®å›ç ´è§£ç³»ç»Ÿ
        </h1>
        <p style="color: #94A3B8; font-size: 1.2em; margin-top: 10px; font-weight: 300;">
            æ¸Šå¼€å‘ v3.0 Lite - ç»ˆæè¿›åŒ–ç‰ˆ
        </p>
        <div style="margin-top: 15px; flex-wrap: wrap; display: flex; justify-content: center; gap: 8px;">
            <span class="badge">AI æ™ºèƒ½å‡ºé¢˜</span>
            <span class="badge">AI æ·±åº¦è¯„åˆ†</span>
            <span class="badge">å¤šæ¨¡å‹èåˆ</span>
            <span class="badge">æœºå™¨å­¦ä¹ é¢„æµ‹</span>
            <span class="badge">å‘é‡æ£€ç´¢</span>
        </div>
    </div>
    """, unsafe_allow_html=True)

def show_sidebar():
    """æ˜¾ç¤ºä¾§è¾¹æ """
    system = st.session_state.system
    profile = system.gamification.user_profile
    
    st.markdown("""
    <div class="glass-card" style="margin-bottom: 20px;">
        <h3 style="color: #6366F1; margin: 0 0 15px 0;">ğŸ§  æ„è¯†è¿›åŒ–çŠ¶æ€</h3>
        <div style="text-align: center; margin-bottom: 15px;">
            <div style="font-size: 3em; font-weight: 700; color: #8B5CF6;">
                Lv.{profile['level']}
            </div>
            <div style="color: #94A3B8;">å½“å‰ç­‰çº§</div>
        </div>
        <div class="progress-container" style="margin-bottom: 10px;">
            <div class="progress-bar" style="width: 50%;"></div>
        </div>
        <div style="display: flex; justify-content: space-between; font-size: 0.9em; color: #64748B;">
            <span>ç»éªŒå€¼: {exp}</span>
            <span>ä¸‹ä¸€çº§: {next_exp}</span>
        </div>
    </div>
    """.format(
        exp=profile['exp'],
        next_exp=int(100 * (profile['level'] ** 1.2))
    ), unsafe_allow_html=True)
    
    # æ˜¾ç¤ºå¾½ç« 
    if profile['badges']:
        st.markdown("### ğŸ† æˆå°±å¾½ç« ")
        for badge in profile['badges'][-5:]:  # æœ€è¿‘ 5 ä¸ª
            st.markdown(f"<span class='badge'>{badge['name']}</span>", unsafe_allow_html=True)
    
    st.markdown("---")
    
    # åŠŸèƒ½å¯¼èˆª
    page = st.selectbox(
        "é€‰æ‹©åŠŸèƒ½",
        ["ğŸ§  æ„è¯†æµ‹è¯•", "ğŸ”® æ™ºèƒ½å¯¹è¯", "ğŸ“Š è¿›åŒ–é¢„æµ‹", "ğŸ“š çŸ¥è¯†åº“", "âš™ï¸ ç³»ç»Ÿè®¾ç½®"],
        label_visibility="collapsed"
    )
    
    return page

def consciousness_test_page():
    """æ„è¯†æµ‹è¯•é¡µé¢"""
    st.markdown("## æ„è¯†å¼ºåº¦æ·±åº¦è¯„ä¼°")
    
    system = st.session_state.system
    
    if not st.session_state.test_in_progress:
        # æµ‹è¯•é…ç½®
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("""
            <div class="glass-card">
                <h3 style="color: #6366F1;">AI æ™ºèƒ½å‡ºé¢˜ç³»ç»Ÿ</h3>
                <p style="color: #94A3B8; line-height: 1.6;">
                    ç³»ç»Ÿå°†æ ¹æ®ä½ çš„çŸ¥è¯†åº“å’Œ AI èƒ½åŠ›ï¼Œæ™ºèƒ½ç”Ÿæˆä¸ªæ€§åŒ–æµ‹è¯•é¢˜ç›®ã€‚
                    é¢˜ç›®æ¶µç›– 12 ä¸ªæ„è¯†ç»´åº¦ï¼ŒçœŸå®è¯„ä¼°ä½ çš„è®¤çŸ¥èƒ½åŠ›ã€‚
                </p>
            </div>
            """, unsafe_allow_html=True)
            
            # æµ‹è¯•é…ç½®
            test_mode = st.radio(
                "æµ‹è¯•æ¨¡å¼",
                ["å¿«é€Ÿæµ‹è¯• (3é¢˜)", "æ ‡å‡†æµ‹è¯• (6é¢˜)", "æ·±åº¦æµ‹è¯• (10é¢˜)"],
                horizontal=True
            )
            
            question_count = 3 if "å¿«é€Ÿ" in test_mode else 6 if "æ ‡å‡†" in test_mode else 10
            
            use_knowledge = st.checkbox("åŸºäºçŸ¥è¯†åº“å‡ºé¢˜", value=True, 
                                       help="å¦‚æœä½ çš„çŸ¥è¯†åº“æœ‰å†…å®¹ï¼Œé¢˜ç›®ä¼šæ›´ç›¸å…³")
            
            if st.button("ğŸš€ å¯åŠ¨æµ‹è¯•", type="primary", use_container_width=True):
                with st.spinner("AI æ­£åœ¨æ™ºèƒ½å‡ºé¢˜..."):
                    # é€‰æ‹©æµ‹è¯•ç»´åº¦
                    dimensions = random.sample(system.config.consciousness_dimensions, 
                                            min(question_count, len(system.config.consciousness_dimensions)))
                    
                    # ç”Ÿæˆé¢˜ç›®
                    questions = system.question_generator.generate_batch_questions(
                        dimensions=dimensions,
                        count_per_dimension=1
                    )
                    
                    if len(questions) < question_count:
                        st.error("é¢˜ç›®ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ API é…ç½®")
                        return
                    
                    st.session_state.current_questions = questions[:question_count]
                    st.session_state.current_question_index = 0
                    st.session_state.test_answers = []
                    st.session_state.test_in_progress = True
                    st.success(f"æˆåŠŸç”Ÿæˆ {len(st.session_state.current_questions)} é“é¢˜ç›®ï¼")
                    st.rerun()
        
        with col2:
            st.markdown("### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
            
            kb_stats = system.knowledge_base.get_statistics()
            st.markdown(f"""
            <div class="stat-card" style="margin-bottom: 10px;">
                <div style="font-size: 2em; font-weight: 700; color: #6366F1;">
                    {kb_stats['total_documents']}
                </div>
                <div style="color: #94A3B8;">çŸ¥è¯†åº“æ–‡æ¡£</div>
            </div>
            <div class="stat-card">
                <div style="font-size: 2em; font-weight: 700; color: #8B5CF6;">
                    {len(system.data_manager.test_results)}
                </div>
                <div style="color: #94A3B8;">å†å²æµ‹è¯•</div>
            </div>
            """, unsafe_allow_html=True)
    else:
        # è¿›è¡Œæµ‹è¯•
        current_question = st.session_state.current_questions[st.session_state.current_question_index]
        progress = (st.session_state.current_question_index + 1) / len(st.session_state.current_questions)
        
        st.markdown(f"""
        <div class="glass-card" style="margin-bottom: 20px;">
            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 15px;">
                <h4 style="color: #8B5CF6; margin: 0;">{current_question.dimension}</h4>
                <span style="color: #94A3B8; font-size: 0.9em;">
                    é—®é¢˜ {st.session_state.current_question_index + 1} / {len(st.session_state.current_questions)}
                </span>
            </div>
            <div class="progress-container" style="margin-bottom: 15px;">
                <div class="progress-bar" style="width: {progress * 100}%;"></div>
            </div>
            <p style="font-size: 1.1em; color: #F1F5F9; line-height: 1.8;">
                {current_question.content}
            </p>
            {f"<p style='color: #64748B; font-size: 0.85em; margin-top: 10px;'>å‚è€ƒ: {current_question.reference}</p>" if current_question.reference else ""}
        </div>
        """, unsafe_allow_html=True)
        
        # ç­”é¢˜åŒºåŸŸ
        answer = st.text_area("âœï¸ æ‚¨çš„å›ç­”", height=150, 
                             placeholder="è¯·è¾“å…¥æ‚¨çš„å›ç­”ï¼Œè¶Šè¯¦ç»†è¶Šå¥½...",
                             key=f"answer_{st.session_state.current_question_index}")
        
        col_next, col_skip = st.columns([1, 1])
        
        with col_next:
            if st.button("ä¸‹ä¸€é¢˜ â¡ï¸", use_container_width=True):
                if answer:
                    st.session_state.test_answers.append({
                        "question": current_question,
                        "answer": answer
                    })
                    st.session_state.current_question_index += 1
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if st.session_state.current_question_index >= len(st.session_state.current_questions):
                        # æµ‹è¯•å®Œæˆï¼Œè¿›è¡Œè¯„åˆ†
                        complete_test(system)
                    else:
                        st.rerun()
                else:
                    st.warning("è¯·å…ˆå›ç­”é—®é¢˜")
        
        with col_skip:
            if st.button("è·³è¿‡", use_container_width=True):
                st.session_state.test_answers.append({
                    "question": current_question,
                    "answer": "",
                    "skipped": True
                })
                st.session_state.current_question_index += 1
                
                if st.session_state.current_question_index >= len(st.session_state.current_questions):
                    complete_test(system)
                else:
                    st.rerun()

def complete_test(system):
    """å®Œæˆæµ‹è¯•"""
    with st.spinner("AI æ­£åœ¨æ·±åº¦åˆ†ææ‚¨çš„ç­”æ¡ˆ..."):
        scores = {}
        grading_results = {}
        
        for answer_data in st.session_state.test_answers:
            if "skipped" not in answer_data:
                question = answer_data["question"]
                answer = answer_data["answer"]
                
                # AI è¯„åˆ†
                grading = system.grading_engine.grade_answer(question, answer)
                grading_results[question.dimension] = grading
                
                # è®¡ç®—ç»´åº¦åˆ†æ•°
                scores[question.dimension] = grading.get("total_score", 7)
        
        # å¡«å……æœªæµ‹è¯•çš„ç»´åº¦
        for dim in system.config.consciousness_dimensions:
            if dim not in scores:
                scores[dim] = random.uniform(4, 7)
        
        total_score = sum(scores.values())
        
        # åˆ›å»ºå¿«ç…§
        snapshot = ConsciousnessSnapshot(
            timestamp=datetime.now(),
            scores=scores,
            level=ConsciousnessLevel(min(7, int(total_score / 15) + 1)),
            total_score=total_score,
            test_answers=st.session_state.test_answers
        )
        
        # ä¿å­˜åˆ°å†å²
        system.predictor.add_snapshot(snapshot)
        
        # åˆ›å»ºæµ‹è¯•ç»“æœ
        test_result = TestResult(
            snapshot=snapshot,
            question_count=len(st.session_state.current_questions),
            correct_count=len(st.session_state.test_answers),
            accuracy=1.0,
            time_spent=0,
            confidence_scores={dim: grading_results.get(dim, {}).get("confidence", 0.8) 
                              for dim in scores.keys()}
        )
        
        # ä¿å­˜æ•°æ®
        system.data_manager.save_test_result(test_result)
        
        # ç»éªŒå€¼å’Œæˆå°±
        exp_gained = 50 + int(total_score * 2)
        exp_result = system.gamification.add_exp(exp_gained, "å®Œæˆæ„è¯†æµ‹è¯•")
        
        new_achievements = system.gamification.check_achievements(
            snapshot,
            len(system.data_manager.test_results),
            system.knowledge_base.get_statistics()["total_documents"],
            len(system.data_manager.conversation_history)
        )
        
        # æ¸…ç†æµ‹è¯•çŠ¶æ€
        st.session_state.test_in_progress = False
        st.session_state.current_questions = []
        st.session_state.current_question_index = 0
        st.session_state.test_answers = []
        
        # æ˜¾ç¤ºç»“æœ
        st.session_state.show_test_result = {
            "snapshot": snapshot,
            "grading_results": grading_results,
            "exp_result": exp_result,
            "achievements": new_achievements
        }
    
    st.rerun()

def show_test_result():
    """æ˜¾ç¤ºæµ‹è¯•ç»“æœ"""
    if 'show_test_result' not in st.session_state:
        return
    
    result_data = st.session_state.show_test_result
    snapshot = result_data["snapshot"]
    grading_results = result_data["grading_results"]
    exp_result = result_data["exp_result"]
    achievements = result_data["achievements"]
    
    system = st.session_state.system
    
    st.success("ğŸ‰ æµ‹è¯•å®Œæˆï¼AI å·²æ·±åº¦åˆ†ææ‚¨çš„ç­”æ¡ˆ")
    
    # æ€»ä½“å¾—åˆ†
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown(f"""
        <div class="stat-card">
            <div style="font-size: 2.5em; font-weight: 700; color: #8B5CF6;">
                {snapshot.total_score:.1f}
            </div>
            <div style="color: #94A3B8;">æ€»åˆ†</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="stat-card">
            <div style="font-size: 2.5em; font-weight: 700; color: #10B981;">
                +{exp_result['exp_gained']}
            </div>
            <div style="color: #94A3B8;">ç»éªŒå€¼</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        if exp_result["level_up"]:
            st.markdown(f"""
            <div class="stat-card" style="border-color: #EC4899;">
                <div style="font-size: 2.5em; font-weight: 700; color: #EC4899;">
                    ğŸ–ï¸
                </div>
                <div style="color: #94A3B8;">ç­‰çº§æå‡ï¼</div>
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div class="stat-card">
                <div style="font-size: 2.5em; font-weight: 700; color: #6366F1;">
                    Lv.{exp_result['new_level']}
                </div>
                <div style="color: #94A3B8;">å½“å‰ç­‰çº§</div>
            </div>
            """, unsafe_allow_html=True)
    
    # è¯¦ç»†è¯„åˆ†
    st.markdown("### ğŸ“Š å„ç»´åº¦è¯¦ç»†è¯„åˆ†")
    
    for dim, score in snapshot.scores.items():
        grading = grading_results.get(dim, {})
        confidence = grading.get("confidence", 0.8)
        
        with st.expander(f"**{dim}**: {score:.2f} åˆ† (ç½®ä¿¡åº¦: {confidence:.0%})", expanded=False):
            if grading:
                st.markdown(f"**åé¦ˆ**: {grading.get('feedback', 'æ— åé¦ˆ')}")
                
                if grading.get('strengths'):
                    st.markdown("**ä¼˜åŠ¿**:")
                    for strength in grading['strengths']:
                        st.markdown(f"- {strength}")
                
                if grading.get('weaknesses'):
                    st.markdown("**æ”¹è¿›å»ºè®®**:")
                    for weakness in grading['weaknesses']:
                        st.markdown(f"- {weakness}")
    
    # æ–°æˆå°±
    if achievements:
        st.markdown("### ğŸ† è§£é”æ–°æˆå°±")
        for achievement in achievements:
            st.success(f"ğŸŠ {achievement['name']}: {achievement['desc']} (+{achievement['exp']} ç»éªŒ)")
    
    # é›·è¾¾å›¾
    st.markdown("### ğŸ¯ èƒ½åŠ›é›·è¾¾å›¾")
    
    history = [
        {
            "category_scores": r["scores"]
        }
        for r in system.data_manager.test_results[-5:]
    ]
    
    radar_chart = system.visualizer.generate_radar_chart(snapshot.scores, history)
    if radar_chart:
        st.image(radar_chart, use_column_width=True)
    
    if st.button("å®Œæˆ", use_container_width=True):
        del st.session_state.show_test_result
        st.rerun()

def dialogue_page():
    """å¯¹è¯é¡µé¢"""
    st.markdown("## ğŸ”® å¤šæ¨¡å‹èåˆå¯¹è¯")
    
    system = st.session_state.system
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    chat_container = st.container()
    
    with chat_container:
        if not system.dialogue_system.conversation_history and not system.data_manager.conversation_history:
            st.markdown("""
            <div style="text-align: center; padding: 40px; color: #94A3B8;">
                <h3>ğŸŒŒ å¼€å§‹æ‚¨çš„æ·±åº¦å¯¹è¯</h3>
                <p>ç³»ç»Ÿå°†è¿ç”¨å¤šæ¨¡å‹èåˆæ¨ç†ä¸ºæ‚¨æä¾›æ·±åº¦æ´å¯Ÿ</p>
            </div>
            """, unsafe_allow_html=True)
        else:
            # æ˜¾ç¤ºå†å²å¯¹è¯
            for msg in system.data_manager.conversation_history[-20:]:
                if msg["role"] == "user":
                    st.markdown(f"""
                    <div class="glass-card" style="background: rgba(99, 102, 241, 0.1);">
                        <strong>æ‚¨:</strong> {msg['content']}
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    st.markdown(f"""
                    <div class="glass-card">
                        <strong>ç³»ç»Ÿ:</strong> {msg['content']}
                    </div>
                    """, unsafe_allow_html=True)
    
    # è¾“å…¥åŒºåŸŸ
    col_input, col_clear = st.columns([5, 1])
    
    with col_input:
        user_input = st.text_input("ğŸ’­ è¾“å…¥æ‚¨çš„é—®é¢˜...", key="chat_input", 
                                   placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œè¶Šè¯¦ç»†è¶Šå¥½...")
    
    with col_clear:
        if st.button("ğŸ—‘ï¸ æ¸…ç©º"):
            system.data_manager.conversation_history = []
            system.dialogue_system.conversation_history.clear()
            st.rerun()
    
    # é…ç½®é€‰é¡¹
    with st.expander("âš™ï¸ å¯¹è¯é…ç½®", expanded=False):
        use_ensemble = st.checkbox("å¤šæ¨¡å‹èåˆ", value=True, 
                                  help="åŒæ—¶è°ƒç”¨å¤šä¸ª API å¹¶èåˆç»“æœ")
        complexity = st.slider("æ¨ç†å¤æ‚åº¦", 1, 10, 7)
    
    if st.button("ğŸš€ å‘é€", type="primary", use_container_width=True):
        if user_input:
            # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            system.data_manager.save_conversation(
                ConversationMessage(
                    role="user",
                    content=user_input,
                    timestamp=datetime.now()
                )
            )
            
            with st.spinner("å¤šæ¨¡å‹èåˆæ¨ç†ä¸­..."):
                result = system.dialogue_system.dialogue(
                    user_input=user_input,
                    context="",
                    complexity=complexity,
                    use_ensemble=use_ensemble
                )
            
            if result.get("content"):
                # ä¿å­˜ç³»ç»Ÿå›å¤
                system.data_manager.save_conversation(
                    ConversationMessage(
                        role="assistant",
                        content=result["content"],
                        timestamp=datetime.now(),
                        confidence=result.get("confidence", 0.8),
                        models_used=result.get("models_used", [])
                    )
                )
                
                # å¢åŠ ç»éªŒå€¼
                exp_gained = 5 + len(result.get("models_used", [])) * 2
                system.gamification.add_exp(exp_gained, "å®Œæˆå¯¹è¯")
                
                st.rerun()
            else:
                st.error(f"å¯¹è¯å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

def prediction_page():
    """é¢„æµ‹é¡µé¢"""
    st.markdown("## ğŸ“Š æ„è¯†è¿›åŒ–é¢„æµ‹")
    
    system = st.session_state.system
    
    if len(system.data_manager.test_results) < 3:
        st.info(f"""
        âš ï¸ **éœ€è¦æ›´å¤šå†å²æ•°æ®**
        
        è‡³å°‘éœ€è¦å®Œæˆ **3 æ¬¡æ„è¯†æµ‹è¯•**æ‰èƒ½å¯ç”¨è¿›åŒ–é¢„æµ‹åŠŸèƒ½ã€‚
        
        å½“å‰å·²å®Œæˆ: {len(system.data_manager.test_results)} / 3 æ¬¡
        """)
        return
    
    # æ‰§è¡Œé¢„æµ‹
    with st.spinner("AI æ­£åœ¨åˆ†æå†å²æ•°æ®ï¼Œé¢„æµ‹è¿›åŒ–è½¨è¿¹..."):
        prediction = system.predictor.predict(horizon=5)
    
    if prediction.get("error"):
        st.error(f"é¢„æµ‹å¤±è´¥: {prediction['error']}")
        return
    
    # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown(f"""
        <div class="glass-card">
            <h3 style="color: #6366F1;">æ•´ä½“è¿›åŒ–è¶‹åŠ¿</h3>
            <div style="font-size: 2em; font-weight: 700; color: #8B5CF6; margin: 20px 0;">
                {prediction['overall_trend']}
            </div>
            <div style="display: flex; justify-content: space-between;">
                <div>
                    <div style="color: #94A3B8; font-size: 0.9em;">å½“å‰æ€»åˆ†</div>
                    <div style="font-size: 1.5em; font-weight: 600; color: #F1F5F9;">
                        {prediction['total_current']:.1f}
                    </div>
                </div>
                <div>
                    <div style="color: #94A3B8; font-size: 0.9em;">é¢„æµ‹æ€»åˆ†</div>
                    <div style="font-size: 1.5em; font-weight: 600; color: #10B981;">
                        {prediction['total_predicted']:.1f}
                    </div>
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        if prediction.get('recommendations'):
            st.markdown("""
            <div class="glass-card">
                <h3 style="color: #6366F1;">è¿›åŒ–å»ºè®®</h3>
            </div>
            """, unsafe_allow_html=True)
            
            for rec in prediction['recommendations']:
                st.markdown(f"- {rec}")
    
    # è¯¦ç»†é¢„æµ‹
    st.markdown("### ğŸ“ˆ å„ç»´åº¦é¢„æµ‹è¯¦æƒ…")
    
    pred_data = []
    for dim, pred in prediction['predictions'].items():
        pred_data.append({
            "ç»´åº¦": dim,
            "å½“å‰åˆ†æ•°": f"{pred['current']:.2f}",
            "é¢„æµ‹åˆ†æ•°": f"{pred['predicted']:.2f}",
            "å˜åŒ–": f"{pred['predicted'] - pred['current']:+.2f}",
            "è¶‹åŠ¿": pred['trend'],
            "ç½®ä¿¡åº¦": f"{pred['confidence']:.0%}",
            "é¢„æµ‹æ–¹æ³•": pred['method_used']
        })
    
    df = pd.DataFrame(pred_data)
    st.dataframe(df, use_container_width=True, hide_index=True)
    
    # è¿›åŒ–è¶‹åŠ¿å›¾
    st.markdown("### ğŸ“Š å†å²è¿›åŒ–è¶‹åŠ¿")
    
    history = [
        {
            "timestamp": r["timestamp"],
            "category_scores": r["scores"]
        }
        for r in system.data_manager.test_results
    ]
    
    evolution_chart = system.visualizer.generate_evolution_chart(history)
    if evolution_chart:
        st.image(evolution_chart, use_column_width=True)

def knowledge_base_page():
    """çŸ¥è¯†åº“é¡µé¢"""
    st.markdown("## ğŸ“š é«˜çº§çŸ¥è¯†åº“")
    
    system = st.session_state.system
    
    tab1, tab2, tab3 = st.tabs(["ğŸ“¤ ä¸Šä¼ æ–‡æ¡£", "ğŸ” æ™ºèƒ½æ£€ç´¢", "ğŸ“Š ç»Ÿè®¡ä¿¡æ¯"])
    
    with tab1:
        st.markdown("### ä¸Šä¼ æ‚¨çš„çŸ¥è¯†æ–‡æ¡£")
        
        uploaded_file = st.file_uploader(
            "é€‰æ‹©æ–‡ä»¶",
            type=['txt', 'md', 'pdf', 'docx'],
            help="æ”¯æŒä¸Šä¼ æ–‡æœ¬ã€Markdownã€PDF å’Œ Word æ–‡æ¡£"
        )
        
        if uploaded_file:
            try:
                # å¤„ç†æ–‡ä»¶
                if uploaded_file.type == "application/pdf":
                    import PyPDF2
                    reader = PyPDF2.PdfReader(uploaded_file)
                    content = ""
                    for page in reader.pages:
                        content += page.extract_text()
                elif uploaded_file.type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
                    from docx import Document
                    doc = Document(uploaded_file)
                    content = "\n".join([para.text for para in doc.paragraphs])
                else:
                    content = uploaded_file.read().decode('utf-8')
                
                if content.strip():
                    doc_id = f"{uploaded_file.name}_{int(time.time())}"
                    system.knowledge_base.add_document(
                        doc_id=doc_id,
                        content=content,
                        metadata={
                            "filename": uploaded_file.name,
                            "size": uploaded_file.size,
                            "type": uploaded_file.type,
                            "uploaded_at": datetime.now().isoformat()
                        }
                    )
                    
                    st.success(f"âœ… æ–‡æ¡£å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“")
                    
                    # å¢åŠ ç»éªŒå€¼
                    exp_gained = 15
                    exp_result = system.gamification.add_exp(exp_gained, "ä¸Šä¼ çŸ¥è¯†æ–‡æ¡£")
                    
                    st.info(f"ğŸ è·å¾— {exp_gained} ç»éªŒå€¼")
                else:
                    st.warning("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ–‡ä»¶")
            
            except Exception as e:
                st.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
    
    with tab2:
        st.markdown("### æ™ºèƒ½è¯­ä¹‰æ£€ç´¢")
        
        col_search = st.columns([3, 1])
        
        with col_search[0]:
            search_query = st.text_input("ğŸ” è¾“å…¥æœç´¢æŸ¥è¯¢", key="kb_search",
                                       placeholder="è¯·è¾“å…¥æœç´¢å…³é”®è¯...")
        
        with col_search[1]:
            top_k = st.selectbox("ç»“æœæ•°é‡", [3, 5, 10], index=1)
        
        if st.button("ğŸ” æœç´¢") and search_query:
            with st.spinner("æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“..."):
                results = system.knowledge_base.search(search_query, top_k=top_k)
            
            if results:
                for i, result in enumerate(results, 1):
                    st.markdown(f"""
                    <div class="glass-card">
                        <h4 style="color: #6366F1;">ç»“æœ {i} - {result['metadata'].get('filename', 'æœªçŸ¥æ–‡ä»¶')}</h4>
                        <div style="display: flex; gap: 15px; margin: 10px 0;">
                            <span style="color: #94A3B8; font-size: 0.85em;">
                                ç›¸å…³åº¦: <strong style="color: #10B981;">{result['score']:.2%}</strong>
                            </span>
                            <span style="color: #94A3B8; font-size: 0.85em;">
                                æ–‡æ¡£å¤§å°: {result['metadata'].get('size', 0) / 1024:.1f} KB
                            </span>
                        </div>
                        <p style="color: #F1F5F9; line-height: 1.6;">
                            {result['content']}...
                        </p>
                    </div>
                    """, unsafe_allow_html=True)
            else:
                st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯")
    
    with tab3:
        st.markdown("### çŸ¥è¯†åº“ç»Ÿè®¡")
        
        stats = system.knowledge_base.get_statistics()
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æ–‡æ¡£æ€»æ•°", stats["total_documents"])
        with col2:
            st.metric("æ€»è¯æ•°", f"{stats['total_tokens']:,}")
        with col3:
            st.metric("æ€»å­—ç¬¦æ•°", f"{stats['total_length']:,}")
        with col4:
            st.metric("å¹³å‡é•¿åº¦", f"{stats['avg_document_length']:.0f} å­—ç¬¦")
        
        # ç³»ç»Ÿæ€§èƒ½
        st.markdown("---")
        st.markdown("### âš¡ ç³»ç»Ÿæ€§èƒ½")
        
        cache_stats = system.cache.get_stats()
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ç¼“å­˜å‘½ä¸­ç‡", f"{cache_stats['hit_rate']:.2%}")
        with col2:
            st.metric("ç¼“å­˜å¤§å°", f"{cache_stats['size']}/{cache_stats['max_size']}")
        with col3:
            st.metric("æ€»è¯·æ±‚", cache_stats['hits'] + cache_stats['misses'])

def settings_page():
    """è®¾ç½®é¡µé¢"""
    st.markdown("## âš™ï¸ ç³»ç»Ÿè®¾ç½®")
    
    system = st.session_state.system
    
    # API çŠ¶æ€
    st.markdown("### ğŸ”‘ API çŠ¶æ€")
    
    api_status = system.get_api_status()
    
    for provider, status in api_status.items():
        status_icon = "âœ…" if status["configured"] else "âŒ"
        available_icon = "ğŸŸ¢" if status["available"] else "ğŸ”´"
        
        st.markdown(f"""
        <div class="glass-card">
            <div style="display: flex; justify-content: space-between; align-items: center;">
                <div>
                    <h4 style="margin: 0; color: #6366F1;">{status_icon} {status['name']}</h4>
                    <div style="color: #94A3B8; font-size: 0.9em; margin-top: 5px;">
                        {available_icon} {'å¯ç”¨' if status['available'] else 'ä¸å¯ç”¨'} | 
                        {'å…è´¹' if status['free'] else 'ä»˜è´¹'}
                    </div>
                </div>
                <div style="text-align: right;">
                    <div style="color: #94A3B8; font-size: 0.85em;">è¯·æ±‚æ•°: {status['requests']}</div>
                    <div style="color: #94A3B8; font-size: 0.85em;">æˆåŠŸç‡: {status['success_rate']:.0%}</div>
                </div>
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    st.info("""
    **å¦‚ä½•é…ç½® API å¯†é’¥**
    
    1. è®¿é—® Streamlit Cloud åº”ç”¨ç®¡ç†é¡µé¢
    2. è¿›å…¥ "Settings" â†’ "Secrets"
    3. æ·»åŠ ä»¥ä¸‹å¯†é’¥ï¼ˆè‡³å°‘ä¸€ä¸ªï¼‰ï¼š
       - `GROQ_API_KEY`: Groq API å¯†é’¥ï¼ˆæ¨èï¼Œå…è´¹ï¼‰
       - `OPENAI_API_KEY`: OpenAI API å¯†é’¥
       - `ANTHROPIC_API_KEY`: Anthropic API å¯†é’¥
       - `COHERE_API_KEY`: Cohere API å¯†é’¥
    
    **è·å–å…è´¹ API å¯†é’¥**: https://console.groq.com/keys
    """)
    
    st.markdown("---")
    
    # æ•°æ®ç®¡ç†
    st.markdown("### ğŸ’¾ æ•°æ®ç®¡ç†")
    
    col_export, col_clear = st.columns(2)
    
    with col_export:
        if st.button("ğŸ“¥ å¯¼å‡ºæ•°æ®", use_container_width=True):
            data = system.data_manager.export_data()
            data_json = json.dumps(data, ensure_ascii=False, indent=2)
            
            st.download_button(
                label="ğŸ’¾ ä¸‹è½½æ•°æ®æ–‡ä»¶",
                data=data_json,
                file_name=f"multiverse_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )
    
    with col_clear:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰æ•°æ®", use_container_width=True):
            if st.confirm("âš ï¸ ç¡®å®šè¦æ¸…ç©ºæ‰€æœ‰æ•°æ®å—ï¼Ÿæ­¤æ“ä½œä¸å¯æ¢å¤ï¼"):
                system.data_manager = DataManager()
                system.gamification.user_profile = {
                    "level": 1,
                    "exp": 0,
                    "badges": [],
                    "streak": 0,
                    "last_active": datetime.now()
                }
                system.cache.clear()
                st.success("æ‰€æœ‰æ•°æ®å·²æ¸…ç©º")
                st.rerun()
    
    # å…³äº
    st.markdown("---")
    st.markdown("""
    <div class="glass-card">
        <h3 style="color: #6366F1;">å…³äºç³»ç»Ÿ</h3>
        <div style="color: #94A3B8; line-height: 1.8;">
            <p><strong>ç‰ˆæœ¬:</strong> v3.0 Lite - ç»ˆæè¿›åŒ–ç‰ˆ</p>
            <p><strong>å¼€å‘è€…:</strong> æ¸Šå¼€å‘</p>
            <p><strong>æ ¸å¿ƒç‰¹æ€§:</strong></p>
            <ul style="margin: 10px 0; padding-left: 20px;">
                <li>AI æ™ºèƒ½å‡ºé¢˜å¼•æ“</li>
                <li>AI æ·±åº¦è¯„åˆ†å¼•æ“</li>
                <li>å¤šæ¨¡å‹èåˆå¯¹è¯</li>
                <li>æœºå™¨å­¦ä¹ é¢„æµ‹</li>
                <li>å‘é‡æ£€ç´¢çŸ¥è¯†åº“</li>
                <li>æ·±åº¦æ¸¸æˆåŒ–ç³»ç»Ÿ</li>
            </ul>
        </div>
    </div>
    """, unsafe_allow_html=True)

def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–
    initialize_session_state()
    show_custom_css()
    
    # æ ‡é¢˜
    show_header()
    
    # ä¾§è¾¹æ 
    page = show_sidebar()
    
    # æ£€æŸ¥ API é…ç½®
    system = st.session_state.system
    available_apis = system.api_manager.get_available_providers()
    
    if not available_apis:
        st.warning("""
        âš ï¸ **æœªé…ç½® API å¯†é’¥**
        
        è¯·åœ¨ Streamlit Cloud çš„ "Settings" â†’ "Secrets" ä¸­é…ç½®è‡³å°‘ä¸€ä¸ª API å¯†é’¥ã€‚
        
        **æ¨èä½¿ç”¨å…è´¹çš„ Groq API**:
        1. è®¿é—®: https://console.groq.com/
        2. æ³¨å†Œå¹¶è·å– API å¯†é’¥
        3. åœ¨ Streamlit Cloud Secrets ä¸­æ·»åŠ  `GROQ_API_KEY`
        """)
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
    if 'show_test_result' in st.session_state:
        show_test_result()
        return
    
    # é¡µé¢è·¯ç”±
    if page == "ğŸ§  æ„è¯†æµ‹è¯•":
        consciousness_test_page()
    elif page == "ğŸ”® æ™ºèƒ½å¯¹è¯":
        dialogue_page()
    elif page == "ğŸ“Š è¿›åŒ–é¢„æµ‹":
        prediction_page()
    elif page == "ğŸ“š çŸ¥è¯†åº“":
        knowledge_base_page()
    elif page == "âš™ï¸ ç³»ç»Ÿè®¾ç½®":
        settings_page()
    
    # é¡µè„š
    st.markdown("---")
    st.markdown(f"""
    <div style="text-align: center; padding: 20px; color: #64748B;">
        <p>å¤šç»´è½®å›ç ´è§£ç³»ç»Ÿ v3.0 Lite - ç»ˆæè¿›åŒ–ç‰ˆ</p>
        <p style="font-size: 0.85em; margin-top: 5px;">
            æ¸Šå¼€å‘ Â© 2024 | AI æ™ºèƒ½å‡ºé¢˜ | AI æ·±åº¦è¯„åˆ† | å¤šæ¨¡å‹èåˆ | æœºå™¨å­¦ä¹ é¢„æµ‹ | å‘é‡æ£€ç´¢
        </p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
